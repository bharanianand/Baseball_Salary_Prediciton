---
title: "**Baseball hitters salary prediction**"
author: "**__Bharani **"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Do we have all the packages ?
```{r loadpackages, warning=FALSE, message=FALSE}
pacman::p_load(ISLR,gbm, tree,rpart, rpart.plot, caret, 
               data.table, MASS, ggplot2,gains,data.table, forecast, leaps, tidyverse,randomForest)
options(digits = 3)
knitr::opts_chunk$set(echo = TRUE, fig.width=12, fig.height=6, fig.path = 'Figs/')
theme_set(theme_classic())
```

# 1. Data Inset
```{r Data Inset}

wseries.df <- Hitters

wseries_NA.df <- wseries.df
wseries.df <- na.omit(wseries.df)
##wseries.df$League <- ifelse( wseries.df$League == 'N',0,1)
#wseries.df$Division <- ifelse( wseries.df$Division == 'W',0,1)
#wseries.df$NewLeague <- ifelse( wseries.df$NewLeague == 'N',0,1)

sum(is.na(Hitters)) 


```
## 59 'NA' records were removed in this process.


# 2 Log transform : Salary
```{r Log transform}
wseries.df$Salary <- log(wseries.df$Salary)
```
## It's difficult to analyze data with high variance such as Salary. Log transformation makes the analysis relatively easy as it scales the data and closely couples the datapoints. Also it helps minimizing the effects of outliers.



# 3 Years ~ Hits - ggplot

```{r ggplot}
mid <- mean(wseries.df$Salary)

ggplot(wseries.df, aes(x=wseries.df$Years,y=wseries.df$Hits,color=Salary)) +  geom_point() + 
   scale_color_gradient2(midpoint=mid, low="blue", mid="green3",high="red", space ="lab" ) + 
     xlab("World Series Years") + 
      ylab("MLB World Series Player Hits") + 
        ggtitle("Does field experience and higher hits pays well ?")

```
## The interesting patters that we found are: a) In general, sportspersons with higher number of hits (more than 150s) are amongst the highest paid. b) In initial years of their careers (1-5 years) sportspersons are paid way less compared to later on in their career.


# 4. Linear regression model

```{r}
wseries.lm <- regsubsets(log(Salary) ~ ., data = wseries.df,method='exhaustive')
names((summary(wseries.lm)))

sum <- summary(wseries.lm)

sum$bic
coef(wseries.lm,7)

plot(wseries.lm,scale="bic")

plot(sum$bic,xlab='Index',ylab='BIC',type='l')


```

# 5. Data Partition

```{r Data Partition}
set.seed(42)
training.index <- sample(1:nrow(wseries.df), 0.8 *(nrow(wseries.df)))
mlb.train <- wseries.df[training.index, ]
mlb.test <- wseries.df[-training.index, ]
mlb.test.salary <- wseries.df[-training.index, "Salary"]
```

# 6. Regression Tree ~ Years + Hits

```{r Regression Tree ~ Years + Hits, color='gold'}


set.seed(42)
regtree <- rpart(log(Salary) ~ Years + Hits, data = mlb.train)
prp(regtree, type = 1, extra = 1, split.font = 2)  
rpart.rules(regtree, cover = TRUE)



```


## 3 rules that give highest salary are:  a) Years > 4.5 + Hits > 88.5  --> So when the number of hits are more than 88.5 and the years of experience is greater than 4.5 years than those sportspersons have highest salaries.  b) Years > 8.5 + Hits < 88.5 --> Second highest paid sportsperson will be those that have years of experience greater than 8.5 and number of hits is less than 88.5.  c) Years < 4.5 + Hits > 150.5 --> If the years of experience is less than 4.5 and number of hits is greater than 150.5 then these sporsperson will be third highest paid.



# 7. Regression Tree ~ All variables

```{r, color = 'blue'}
set.seed(42)
mlb.tree <- tree(Salary~ ., mlb.train, subset =train)
summary(mlb.tree)

plot(mlb.tree)
text(mlb.tree,pretty=0)

# Boosting for different lambdas
lambdas <- c(c(), seq(0.001, 0.2, by= 0.001)) #Starting lambda with default value 0.01

len_lambdas <- length(lambdas)
MSE.train <- rep(NA,len_lambdas)
MSE.test <- rep(NA,len_lambdas)
for( i in 1:len_lambdas)
{ boost.mlb<-gbm(Salary~., data=mlb.train, distribution = "gaussian", 
                  n.trees = 1000, interaction.depth = 4,
                  shrinkage = lambdas[i], verbose = F)
  mlb.boost.pred.train <-predict(boost.mlb, mlb.train,n.trees = 1000)
  mlb.boost.pred.test <-predict(boost.mlb, mlb.test,n.trees = 1000)
  MSE.train[i] <- mean((mlb.boost.pred.train - mlb.train$Salary)^2) 
  MSE.test[i] <- mean ((mlb.boost.pred.test - mlb.test$Salary)^2)
}


# Plotting of different lambdas ~ MSE for training data
ggplot(data.frame(x=lambdas,y= MSE.train), aes(x=x,y=y)) + geom_point() + geom_smooth(method=glm)+ xlab("Lambdas") + ylab("Training data MSE ")

```

# 8. plot(Lambdas ~ MSE)
```{r}
# Plotting of different lambdas ~ MSE for test data
set.seed(42)    
ggplot(data.frame(x=lambdas,y= MSE.test), aes(x=x,y=y)) + geom_point() + geom_smooth(method=glm)+ xlab("Lambdas") + ylab("Test data MSE")
```

# 9. What did the boosted model say about the predictors ?

```{r}

# Since we ran boosting for various shrinkage parameters,
# Let's take the lambda with least error from the test data.

mlb.boost.train <- gbm(Salary ~ ., data = mlb.train, distribution = "gaussian", 
    n.trees = 1000, shrinkage = lambdas[which(MSE.test == min(MSE.test))])

summary(mlb.boost.train)

```
## From the boosted model's relative influence plot : CAtBat


# 10. Bagging
```{r}
set.seed(42)
bagging <- randomForest(mlb.train$Salary~., data=mlb.train, SSmtry = 19, importance = TRUE,ntree=1000)
bagging.prediction <- predict(bagging, mlb.test)

mean((bagging.prediction-mlb.test$Salary)^2)


```
